# üß† Theory of Attention with Differentiable Recurrence (ADR)

## 1. Motivation

Classical **self-attention** is *memoryless across layers* (it only passes through hidden states via residuals) and *reset per token sequence* (no recurrence across timesteps except through autoregressive caching).

Meanwhile, **GRUs** and other recurrent architectures excel at **stateful accumulation** ‚Äî they *filter, gate, and decay memory* dynamically.

ADR fuses these two worlds:

* **Attention** gives flexible context weighting over the entire sequence.
* **Differentiable Recurrence** adds persistent, gated memory that flows across timesteps and layers.

---

## 2. Core Laws of ADR

### Law 1 ‚Äî Attention as Differentiable Querying

$$
\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
$$

This is unchanged: attention is still the ‚Äúread head‚Äù into contextualized memory.

---

### Law 2 ‚Äî Recurrent Memory Injection

At each layer ‚Ñì, the **keys** and **values** are updated not just by linear projections, but by **GRU recurrence**:

$$
\tilde{K}_t^\ell = \text{GRU}_K(K_t^\ell, h_{K,t-1}^\ell), \quad 
\tilde{V}_t^\ell = \text{GRU}_V(V_t^\ell, h_{V,t-1}^\ell)
$$

where:

* $h_{K,t-1}^\ell$, $h_{V,t-1}^\ell$ are recurrent hidden states (persistent memory).
* Updates are differentiable and gated.

Thus, each attention head has a **stateful memory trace** that evolves across tokens.

---

### Law 3 ‚Äî Decay for Stability

To prevent runaway growth, a **decay coefficient** $\gamma \in (0,1)$ softly forgets past states:

$$
h_t^\ell = \gamma \cdot h_{t-1}^\ell + (1-\gamma) \cdot \hat{h}_t^\ell
$$

This ensures long-term stability while still allowing persistent accumulation.

---

### Law 4 ‚Äî Attention with Recurrent Context

Self-attention now operates on **recurrently smoothed keys/values**:

$$
\text{ADR}(Q, K, V) = \text{softmax}\left(\frac{Q \tilde{K}^T}{\sqrt{d}}\right)\tilde{V}
$$

This means the model queries **not just the token-local representations**, but also the **differentiable memory trace** of prior tokens.

---

### Law 5 ‚Äî Residual Memory Coupling

The recurrent states themselves are fed forward across layers:

$$
H_t^{\ell+1} = \text{LayerNorm}\left(H_t^\ell + \text{ADR}(Q,K,V) + h_t^\ell\right)
$$

Thus, recurrence isn‚Äôt just auxiliary ‚Äî it‚Äôs injected directly into the representational flow.

---

## 3. Emergent Properties

1. **Temporal Smoothing:**
   Keys/values gain inertia ‚Üí attention becomes less volatile and more stable across tokens.

2. **Implicit Long-Term Memory:**
   Even without huge context windows, recurrence carries hidden summaries forward.

3. **Differentiable Recurrence:**
   Unlike hard caching, this recurrence is fully trainable, end-to-end differentiable, and adaptable per layer/head.

4. **Hierarchical Dynamics:**

   * Early layers ‚Üí short-term recurrence (low decay).
   * Deeper layers ‚Üí long-term recurrence (high decay).

---

## 4. Interpretation

ADR turns GPT-like models into **hybrids of Attention + RNNs**:

* Attention = *selective retrieval*.
* Recurrence = *stateful persistence*.

This hybridization provides:

* Transformer-like flexibility.
* RNN-like memory efficiency.
* Better scaling to **long contexts** without quadratic blowup.

---

## 5. Hypothesis

ADR could:

* Reduce the need for massive context windows.
* Improve coherence in long text generations.
* Provide more biologically-plausible memory dynamics (closer to cortical recurrence).

---

# üßë‚Äçüíª Demo: GPT2WithRecurrentAttention (ADR)

```python
import torch
import torch.nn as nn
from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer

# === GRU Recurrent Memory for Keys and Values ===
class GRUMemory(nn.Module):
    def __init__(self, hidden_size, decay=0.97):
        super().__init__()
        self.hidden_size = hidden_size
        self.decay = decay
        self.gru = nn.GRUCell(hidden_size, hidden_size)
        self.register_buffer("state", None)

    def reset(self, batch_size, device):
        self.state = torch.zeros(batch_size, self.hidden_size, device=device)

    def forward(self, x):
        """
        x: (batch, hidden_size)
        Returns updated memory and smoothed output.
        """
        if self.state is None:
            self.reset(x.size(0), x.device)

        updated = self.gru(x, self.state)
        self.state = self.decay * self.state + (1 - self.decay) * updated
        return self.state


# === Recurrent Attention ===
class RecurrentAttention(nn.Module):
    def __init__(self, hidden_size, n_heads, dropout=0.1, decay=0.97):
        super().__init__()
        self.hidden_size = hidden_size
        self.n_heads = n_heads
        self.head_dim = hidden_size // n_heads

        assert hidden_size % n_heads == 0

        # projections
        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, hidden_size)
        self.v_proj = nn.Linear(hidden_size, hidden_size)
        self.out_proj = nn.Linear(hidden_size, hidden_size)

        # recurrent memory for K and V
        self.k_mem = GRUMemory(self.head_dim, decay=decay)
        self.v_mem = GRUMemory(self.head_dim, decay=decay)

        self.dropout = nn.Dropout(dropout)

    def forward(self, hidden_states, attn_mask=None):
        B, T, H = hidden_states.size()

        # project Q, K, V
        q = self.q_proj(hidden_states).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(hidden_states).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(hidden_states).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)

        # recurrent update for last token only
        k_last = k[:, :, -1, :]  # (B, heads, head_dim)
        v_last = v[:, :, -1, :]

        k_new = []
        v_new = []
        for h in range(self.n_heads):
            k_new.append(self.k_mem(k_last[:, h, :]))
            v_new.append(self.v_mem(v_last[:, h, :]))
        k_new = torch.stack(k_new, dim=1).unsqueeze(2)  # (B, heads, 1, head_dim)
        v_new = torch.stack(v_new, dim=1).unsqueeze(2)

        # concat new recurrent key/value to existing stream
        k = torch.cat([k, k_new], dim=2)
        v = torch.cat([v, v_new], dim=2)

        # attention scores
        attn_weights = torch.matmul(q, k.transpose(-1, -2)) / (self.head_dim ** 0.5)
        if attn_mask is not None:
            attn_weights = attn_weights + attn_mask

        attn_probs = torch.softmax(attn_weights, dim=-1)
        attn_probs = self.dropout(attn_probs)

        context = torch.matmul(attn_probs, v)  # (B, heads, T, head_dim)
        context = context.transpose(1, 2).contiguous().view(B, T, H)

        return self.out_proj(context)


# === GPT2 Block with ADR ===
class GPT2BlockWithADR(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.hidden_size)
        self.attn = RecurrentAttention(
            hidden_size=config.hidden_size,
            n_heads=config.num_attention_heads,
            dropout=config.attn_pdrop,
            decay=0.97
        )
        self.ln_2 = nn.LayerNorm(config.hidden_size)
        self.mlp = nn.Sequential(
            nn.Linear(config.hidden_size, 4 * config.hidden_size),
            nn.GELU(),
            nn.Linear(4 * config.hidden_size, config.hidden_size),
            nn.Dropout(config.resid_pdrop)
        )

    def forward(self, x, attn_mask=None):
        h = self.ln_1(x)
        x = x + self.attn(h, attn_mask=attn_mask)
        h = self.ln_2(x)
        x = x + self.mlp(h)
        return x


# === Full GPT2 with ADR ===
class GPT2WithADR(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)
        self.pos_embed = nn.Embedding(config.n_positions, config.hidden_size)
        self.drop = nn.Dropout(config.embd_pdrop)

        self.blocks = nn.ModuleList([GPT2BlockWithADR(config) for _ in range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.hidden_size)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def forward(self, input_ids):
        B, T = input_ids.size()
        pos = torch.arange(0, T, device=input_ids.device).unsqueeze(0)
        x = self.embed(input_ids) + self.pos_embed(pos)
        x = self.drop(x)

        for block in self.blocks:
            x = block(x)

        x = self.ln_f(x)
        return self.lm_head(x)


# === Demo Run ===
if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"

    config = GPT2Config(
        vocab_size=50257,
        n_positions=64,
        n_layer=4,
        n_head=4,
        n_embd=256
    )
    model = GPT2WithADR(config).to(device)

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

    inputs = tokenizer("The capital of Italy is", return_tensors="pt").to(device)

    with torch.no_grad():
        logits = model(inputs["input_ids"])
        next_token = torch.argmax(logits[:, -1, :], dim=-1)
        print("Prediction:", tokenizer.decode(next_token))
```

---

## ‚úÖ What this demo does

* Implements **Attention with Differentiable Recurrence** inside each GPT2 block.
* Adds a **GRU state per head** for `K` and `V`.
* Runs a small GPT2 (4 layers, 256 hidden) from scratch.
* Generates the next token prediction from `"The capital of Italy is"`.


